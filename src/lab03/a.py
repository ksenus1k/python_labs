def normalize(text: str, *, casefold: bool = True, yo2e: bool = True) -> str:
    text=text.casefold()
    if yo2e:
        text=text.replace('Ñ‘','Ðµ').replace('Ð','Ð•')
    text=text.replace('\t',' ').replace('\r',' ').replace('\n',' ')
    text=' '.join(text.split())
    text=text.strip()
    return text
print(normalize("ÐŸÑ€Ð˜Ð²Ð•Ñ‚\nÐœÐ˜Ñ€\t"))
print(normalize("Ñ‘Ð¶Ð¸Ðº, ÐÐ»ÐºÐ°"))
print(normalize("Hello\r\nWorld"))
print(normalize("  Ð´Ð²Ð¾Ð¹Ð½Ñ‹Ðµ   Ð¿Ñ€Ð¾Ð±ÐµÐ»Ñ‹  "))

import re
def tokenize(text: str) -> list[str]:
    return re.findall(r'\w+(?:-\w+)*',text)
print(tokenize("Ð¿Ñ€Ð¸Ð²ÐµÑ‚ Ð¼Ð¸Ñ€"))
print(tokenize("hello,world!!!"))
print(tokenize("Ð¿Ð¾-Ð½Ð°ÑÑ‚Ð¾ÑÑ‰ÐµÐ¼Ñƒ ÐºÑ€ÑƒÑ‚Ð¾"))
print(tokenize("2025 Ð³Ð¾Ð´"))
print(tokenize("emoji ðŸ˜€ Ð½Ðµ ÑÐ»Ð¾Ð²Ð¾"))

def count_freq(tokens: list[str]) -> dict[str, int]:
    dictionary={}
    for i in tokens:
        value=dictionary.get(i,0)
        dictionary[i]=value+1
    return dictionary

def top_n(freq: dict[str, int], n: int = 5) -> list[tuple[str, int]]:
    s=[]
    for word, count in freq.items():
        s.append((-count,word))
    s.sort()
    result=[]
    for neg_count, word in s:
        result.append((word,-neg_count))
    return result[:n]
print(count_freq(["a","b","a","c","b","a"]))
print(count_freq(["bb","aa","bb","aa","cc"]))
print(top_n(count_freq(["a","b","a","c","b","a"])))
print(top_n(count_freq(["bb","aa","bb","aa","cc"])))